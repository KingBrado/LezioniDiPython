{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rete Neurale: Classificazione di fiori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo notebook descriverà passo passo ciò che si fa per allenare una vera e propria rete neurale (per quanto semplice). Rispetto al primo esempio, non faremo l'assunzione di dipendenza lineare e, di conseguenza, dovremo usare un approccio più generale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di tutto, descrivo qual è il problema che cerchiamo di risolvere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris è un tipo di fiore e, in base alla lunghezza del petalo e del sepalo, si può classificare in tre sottospecie. Di seguito uso Pillow e Matplotlib per visualizzare un esempio di ciascuna \"classe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`os` è un pacchetto che serve ad astrarre operazioni dei sistemi operativi. Per esempio su sistemi non Windows, come anche in tutti i principali linguaggi di programmazione, un percorso di un file in una cartella si scrive come `cartella/file`, mentre Windows usa `cartella\\\\file` (La doppia sbarra serve perché in una stringa '\\\\' è un carattere speciale). Usando `os` deleghiamo la gestione di operazioni specifiche a Python, e non ce ne preoccupiamo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'interno di questa cartella (`RetiNeurali`) ci sono una cartella `dati`, che contiene dati per i nostri problemi, e una cartella `immagini`, che contiene, be', immagini. Dunque, per aprire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_immagini = os.path.join('immagini') # Equivale a .\\\\immagini su Windows, ./immagini su Unix.\n",
    "nomi_immagini = os.listdir(percorso_immagini) # listdir crea una lista con l'elenco dei file in percorso_immagini\n",
    "print(nomi_immagini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora leggiamo le immagini\n",
    "virginica = Image.open(os.path.join(percorso_immagini, nomi_immagini[2]))\n",
    "setosa = Image.open(os.path.join(percorso_immagini, nomi_immagini[3]))\n",
    "versicolor = Image.open(os.path.join(percorso_immagini, nomi_immagini[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax[0].imshow(virginica)\n",
    "ax[0].set_title('Iris Virginica')\n",
    "ax[1].imshow(setosa)\n",
    "ax[1].set_title('Iris Setosa')\n",
    "ax[2].imshow(versicolor)\n",
    "ax[2].set_title('Iris Versicolor')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La specie può essere determinata misurando 4 quantità: lunghezza del sepalo, larghezza del sepalo, lunghezza del petalo e larghezza del petalo. Per fortuna qualcuno, nel 1936, ha preso le misure di 150 fiori, e i dati sono disponibili per tutti. Infatti, nella cartella `dati`, c'è un file .csv (Comma separated value), che non è altro che una lista di numeri divisi da una virgola. La prima riga può (opzionale) contenere nomi delle varie colonne. Ogni riga successiva alla prima contiene esattamente lo stesso numero di valori.  \n",
    "Potremmo usare le solite funzionlità di lettura dei file di Python, ma il pacchetto `pandas` aiuta con questo tipo di file, oltre a facilitare visualizzazione ed alcune operazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris = pd.read_csv(os.path.join('dati', 'iris.csv')) # Crea un dataframe\n",
    "iris.head() # Il metodo head() visualizza le prime n righe, default 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La quinta colonna in particolare definisce la classe della varietà. Cerchiamo di estrarre tutti i possibili valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varieta = iris['variety']\n",
    "varieta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È una lista con valori ripetuti meglio passarla a un `set` (insieme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varieta = set(varieta)\n",
    "varieta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vedi, un `set` non contiene mai duplicati, in linea con la definizione di insieme nel senso matematico. Quindi aggiungendo un elemento che esiste già non succede niente. Nota anche le parentesi graffe (come un dizionario, ma senza coppie di valori chiave: valore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto cerchiamo di analizzare graficamente se c'è una relazione ovvia fra le quantità. Essendo le variabili 4, le rappresento a due a due."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titoli = iris.keys()[:-1] # keys dà la riga con i nomi delle colonne; variety non ci serve al momento.\n",
    "fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n",
    "colori = ['red','green','blue']\n",
    "for riga, titolo in enumerate(titoli):\n",
    "    for colonna, titolo_2 in enumerate(titoli):\n",
    "        if titolo == titolo_2:\n",
    "            ax[riga][colonna].text(0.5, 0.5, titolo.title(), horizontalalignment='center', fontsize=20)\n",
    "        else:\n",
    "            for colore, var in zip(colori, varieta):\n",
    "                specie = iris[iris['variety']==var]\n",
    "                ax[riga][colonna].scatter(specie[titolo], specie[titolo_2], color=colore, label=var)\n",
    "            ax[riga][colonna].legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una relazione c'è ma non sembra esserci una legge lineare e in alcuni casi i dati non sembrano molto separabili. Cerchiamo dunque una relazione generica (non lineare), in forma di rete neurale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rete Neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una rete neurale è, come dice il nome, una rete di neuroni. Il concetto prende ispirazione dai neuroni naturali, in particolare, quelli che formano il cervello umano. Un neurone riceve segnali elettrici dagli altri neuroni attraverso i *dendriti*. La corrente porta cariche al neurone che si accumulano fino a una soglia limite, oltre la quale il neurone genera una corrente di *attivazione* lungo l'assione *assione*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone = Image.open(os.path.join(percorso_immagini, nomi_immagini[1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.imshow(neurone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo modello di neurone (naturale) riportato in letteratura descriveva il funzionamento come una somma di segnali provenienti dai neuroni precedenti, passato attraverso una *funzione di attivazione*, che nel caso particolare era la funzione $\\theta$:  \n",
    "  \n",
    "  $\\theta = \\begin{cases} 1,\\quad \\mbox{se }x \\ge 0\\\\\n",
    "                          0,\\quad \\mbox{altrimenti}\n",
    "            \\end{cases}$  \n",
    "  \n",
    "In pratica questo modello del neurone è molto semplice, ma ha dato modo di sviluppare modelli più complessi e, dunque, realistici. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da un punto di vista matematico, il concetto è però rimasto interessante, considerando che ha dato origine al fenomeno delle *reti neurali artificiali*. Scrivo quindi la prima classe neurone, per illustrare come funziona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lineare(x: float) -> float:\n",
    "    return x\n",
    "\n",
    "def theta(x: float) -> float:\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "class Neurone:\n",
    "    def __init__(self, attivazione: str ='theta'):\n",
    "        self.w = np.random.random()\n",
    "        self.b = np.random.random()\n",
    "        self.attivazione = theta if attivazione == 'theta' else lineare\n",
    "        \n",
    "    def predict(self, x: float) -> float:\n",
    "        return self.attivazione(self.w*x+self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiamo dalla classe neurone: il costruttore prende come argomento una stringa che definisce l'attivazione, e inizializza i parametri come numeri aleatori da 0 a 1 (come nell'esempio delle temperature). Inoltre, in base alla stringa, scegli la teta o niente (lineare) come funzione di attivazione (Nota: self.attivazione è una funzione). Durante la predizione, calcoliamo la *retta* e ci applichiamo la nostra funzione di attivazione. Provando ad eseguire la seguente cella più volte, il risultato cambia, ma resta sempre 0 o 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone = Neurone()\n",
    "neurone.predict(-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo detto, però, che un neurone naturale prende come input vari segnali, non un solo numero. Modifichiamo dunque la classe di cui sopra per accettare un vettore di segnali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurone:\n",
    "    def __init__(self, dim_input, attivazione: str ='theta'):\n",
    "        self.w = np.random.random((dim_input,)) # weight\n",
    "        self.b = np.random.random((dim_input,)) # bias\n",
    "        self.attivazione = theta if attivazione == 'theta' else lineare\n",
    "        \n",
    "    def predict(self, x: np.array) -> float:\n",
    "        return self.attivazione(np.sum(self.w*x+self.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora la dimensione va specificata nel costruttore. Inoltre, considerando che moltiplichiamo $w$ e $b$ per l'input, la dimensione deve essere aggiustata di conseguenza. Infine, ci aspettiamo che il neurone restituisca un solo output (sempre per analogia con i neuroni naturali). Sommiamo dunque tutti i termini del vettore risultante dalla somma vettoriale e passiamo il risultato alla funzione di attivazione.  \n",
    "Facciamo un esempio con `dim_input=4` (visto che il nostro dataset ha quattro variabili)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone = Neurone(4)\n",
    "neurone.predict(-1*np.ones((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pratica, la funzione di attivazione $f(x)=\\theta(x)$ non viene usato nelle reti neurali odierne. Questo principalmente perchè fa un salto repentino e la derivata è infinita in 0. In base all'applicazione, le piû frequenti sono:  \n",
    "  \n",
    "  $\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n",
    "  \n",
    "  $\\mbox{logistic}(x)=\\frac{1}{1+e^{-x}}$ \n",
    "  \n",
    "  $\\mbox{ReLU}(x) = \\max(x, 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui riportiamo tutta, comprese la $\\theta(x)$, per completezza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "y = np.heaviside(x, 0)\n",
    "ax[0][0].plot(x, y)\n",
    "ax[0][0].set_title('\\u03B8', fontsize='xx-large', color='lightblue')\n",
    "ax[0][0].set_xlabel('x',fontsize='xx-large', color='lightblue')\n",
    "ax[0][0].set_ylabel('y',fontsize='xx-large', color='lightblue')\n",
    "y = np.tanh(x)\n",
    "ax[0][1].plot(x, y)\n",
    "ax[0][1].set_title('tanh', fontsize='xx-large', color='lightblue')\n",
    "ax[0][1].set_xlabel('x', fontsize='xx-large', color='lightblue')\n",
    "ax[0][1].set_ylabel('y', fontsize='xx-large', color='lightblue')\n",
    "y = 1./(1+np.exp(-x))\n",
    "ax[1][0].plot(x, y)\n",
    "ax[1][0].set_title('logistic', fontsize='xx-large', color='lightblue')\n",
    "ax[1][0].set_xlabel('x',fontsize='xx-large', color='lightblue')\n",
    "ax[1][0].set_ylabel('y',fontsize='xx-large', color='lightblue')\n",
    "y = np.maximum(x, np.zeros_like(x))\n",
    "ax[1][1].plot(x, y)\n",
    "ax[1][1].set_title('ReLU', fontsize='xx-large', color='lightblue')\n",
    "ax[1][1].set_xlabel('x', fontsize='xx-large', color='lightblue')\n",
    "ax[1][1].set_ylabel('y', fontsize='xx-large', color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esempio continuerò a usare la logistica, perchè ha una derivata continua ma, al contrario della tanh, più semplice da calcolare (non solo per noi, ma anche per il pc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto abbiamo tutti gli elementi per una rete neurale, eccetto che un neurone singolo non fa proprio da rete neurale. Infatti, un rete è composta da una serie di neuroni, come nell'esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x: float)-> float:\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "class Neurone:\n",
    "    def __init__(self, dim_input, attivazione: str ='logistic'):\n",
    "        self.w = np.random.random((dim_input,))\n",
    "        self.b = np.random.random((dim_input,))\n",
    "        self.attivazione = logistic if attivazione == 'logistic' else lineare\n",
    "        \n",
    "    def predict(self, x: np.array) -> float:\n",
    "        return self.attivazione(np.sum(self.w*x+self.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input: int, n_output: int, attivazione: str = 'logistic'):\n",
    "        self.neurons = [Neurone(n_input, attivazione) for _ in range(n_output)]\n",
    "        \n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        y = np.array([neuro.predict(x) for neuro in self.neurons])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReteNeurale:\n",
    "    def __init__(self, n_input: int, n_output: int = 1):\n",
    "        # IMPORTANTE!!! L'output di un layer deve essere uguale all'input del successivo\n",
    "        self.layers = [Layer(n_input, 5), Layer(5, n_output, '')]\n",
    "    \n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        y = x \n",
    "        for layer in self.layers:\n",
    "            y = layer.predict(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ReteNeurale(4, )\n",
    "network.predict(np.random.random((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo è un esempio completo del passaggio in avanti (*forward step*) di un rete neurale. Cerchiamo di capire cosa succede:\n",
    "- La classe `ReteNeurale` costruisce l'architettura della rete a partire di *strati* (layers) di neuroni. Nell'esempio ci sono due strati: Il primo prende 4 input (determinato dai dati), e resitutisce 5 output, mentre il secondo prende quei 5 input e restituisce 1 output, un numero che dovrà essere \"vicino\" a 0, 1, 2 (le classi del dataset).\n",
    "- La classe Layer raggruppa tutti i neuroni che prendono lo stesso input, e restituisce tanti output quanto sono i neuroni di cui è composto. Inoltre specifica l'attivazione che ogni strato di neuroni deve usare (neuroni nello stesso strato hanno tutti la stessa).\n",
    "- Il neurone è analogo a quello precedente, ma uso l'attivazione logistica, come detto prima.\n",
    "\n",
    "La figura in basso rappresenta lo schema della rete definita sopra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone = Image.open(os.path.join(percorso_immagini, nomi_immagini[0]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.imshow(neurone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordiamoci ora cosa succedeva nell'esempio della retta: i dati venivano passati alla \"rete\" (la nostra funzione retta), con i parametri inizializzati a caso. Questi andavano poi ottimizzati passando tutti i possibili valori, varie volte o *epoche*. In questo esempio, l'approccio è lo stesso ma ovviamente il calcolo è più complesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo ora come svolgere il calcolo per la funzione espressa tramite la rete neurale. Chiamiamo $x_i$ gli input, con $i\\in \\{1,2,3,4\\}$ nel nostro caso specifio. Ogni neurone prende ciascuno degli input e lo moltiplica per un peso $w_{ij}$ e lo somma per un *bias* $b_i$. Per esempio, il neurone 1 del primo strato calcola la seguente quantità:  \n",
    "  \n",
    "$h_1 = w_{11}x_{1} + b_{11} + w_{12}x_{2} + b_{12} + w_{13}x_{3} + b_{13} + w_{14}x_{4} + b_{14} = \\sum_{i=1}^{4}\\left(w_{1i}x_{i}+b_{1i}\\right)$\n",
    "  \n",
    "Questa quantità deve poi essere passata attraverso la nostra funzione di attivazione:  \n",
    "  \n",
    "  $a_1 = \\mbox{logistic}(h_1)$   \n",
    "  \n",
    "Ogni neurone del primo strato effettua un calcolo analogo, per cui avremo:  \n",
    "  \n",
    "  $a_{i}=\\mbox{logistic}(h_i) = \\mbox{logistic}\\left(\\sum_{j=1}^{4}\\left(w_{ij}x_{j}+b_{ij}\\right)\\right)$  \n",
    " \n",
    "Il neurone di output farà un calcolo simile prendendo come input le $a_i$:  \n",
    "  \n",
    "  $o = \\sum_{i=1}^{4}\\left(w_{oi}a_{i}+b_{oi}\\right) = \\sum_{i=1}^{4}\\left(w_{oi}\\left(\\mbox{logistic}\\left(\\sum_{j=1}^{4}\\left(w_{ij}x_{j}+b_{ij}\\right)\\right)\\right)+b_{oi}\\right)$  \n",
    "  \n",
    "In questo caso non usiamo una funzione di attivazione, quindi questo sarà l'output finale. Ogni volta che facciamo il passo *forward* (forward propagation), verrà calcolata questa funzione. \n",
    "\n",
    "Ovviamente, per imparare qualcosa, i parametri della rete devono essere aggiornati in base al risultato atteso. Calcoliamo quindi la *loss function*:  \n",
    "  \n",
    "  $L(o) = \\left(o - y\\right)^2$\n",
    "  \n",
    "dove le $y$ sono le classi attese. Minimizzare questa funzione permette di determinare i parametri che ottimizzano la funzione di cui sopra. Calcoliamo dunque le derivate della loss function in funzione dei vari parametri. Notiamo che ogni neurone ha 8 parametri (4 $w_{ij}$ e  4 $b_{ij}$), per un totale di $6\\cdot8=48$ parametri. Cominciamo dalle derivate rispetto ai parametri del neurone di output:  \n",
    "  \n",
    "  $\\frac{\\partial L}{\\partial w_{oi}} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial w_{oi}} = 2\\left(o - y\\right)a_i$  \n",
    "  \n",
    "  In questa relazione abbiamo calcolato, nel primo passaggio, la derivata composta $f(g(x))$, notando che $L(o)$ è semplicemente la derivata di una potenza. La derivata di $o$ in funzione di $w_{ij}$ è poi la derivata di una somma di cui un solo termine contiene la variabile $i$-esima, rendendo gli altri termini nulli (derivata di una costante). La derivata rispetto a $b_{oi}$ sarà simile eccetto per il prodotto per $a_i$ che non ci sarà.\n",
    "  \n",
    "Le derivate rispetto agli $w_{ij}$ e $b_{ij}$ saranno relativamente più complesse:  \n",
    "  \n",
    "  $\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial a_{i}}\\frac{\\partial a_i}{\\partial h_{i}}\\frac{\\partial h_i}{\\partial w_{ij}}$\n",
    "  \n",
    "Il primo fattore è identico al primo fattore dell'espressione precedente, mentre il secondo sostituisce $a_i$ con $w_{oi}$. Il terzo fattore è la derivata della funzione logistica:  \n",
    "  \n",
    "  $\\frac{\\partial}{\\partial h_i} \\mbox{logistic} (h_i)= \\frac{\\partial}{\\partial h_i} \\left(1+e^{-h_i}\\right)^{-1} = -\\left(1+e^{-h_i}\\right)^{-2} (-1) e^{-h_i} = \\frac{e^{-h_i}}{\\left(1+e^{-h_i}\\right)^{2}} = e^{-h_i}\\left(\\mbox{logistic}(h_i)\\right)^2$\n",
    "  \n",
    "  \n",
    "Quindi l'espressione finale sarà:  \n",
    "  \n",
    "  $\\frac{\\partial L}{\\partial w_{ij}}= 2\\left(o - y\\right)w_{oi}e^{-h_i}\\left(\\mbox{logistic}(h_i)\\right)^2x_j$\n",
    "  \n",
    "Proviamo a implementare queste relazioni in codice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineare(x: float) -> float:\n",
    "    return x\n",
    "\n",
    "def der_lineare(x: float) -> float:\n",
    "    return 1\n",
    "\n",
    "def logistic(x: float)-> float:\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def der_logistic(x: float) -> float:\n",
    "    return np.exp(-x)*logistic(x)**2\n",
    "\n",
    "class Neurone:\n",
    "    def __init__(self, dim_input, attivazione: str ='logistic'):\n",
    "        self.w = np.random.random((dim_input,))\n",
    "        self.b = np.random.random((dim_input,))\n",
    "        self.attivazione = logistic if attivazione == 'logistic' else lineare\n",
    "        self.der_attivazione = der_logistic if attivazione == 'logistic' else der_lineare\n",
    "        \n",
    "    def predict(self, x: np.array) -> float:\n",
    "        self.input = x\n",
    "        self.h = np.sum(self.w*x+self.b)\n",
    "        return self.attivazione(self.h)\n",
    "    \n",
    "    def update(self, grad: float, lr: float) -> np.array:\n",
    "        grad = self.der_attivazione(self.h)*np.sum(grad)\n",
    "        output = grad * self.w\n",
    "        self.w -= lr * grad * self.input\n",
    "        self.b -= lr * grad * np.ones_like(self.b)\n",
    "        return output\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input: int, n_output: int, attivazione: str = 'logistic'):\n",
    "        self.neurons = [Neurone(n_input, attivazione) for _ in range(n_output)]\n",
    "        \n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        y = np.array([neuro.predict(x) for neuro in self.neurons])\n",
    "        return y\n",
    "    \n",
    "    def update(self, grad: np.array, lr: float) -> np.array:\n",
    "        grad = [neuron.update(grad[:, neuro_id], lr)\n",
    "                for neuro_id, neuron in enumerate(self.neurons)]\n",
    "        grad = np.stack(grad)\n",
    "        return grad\n",
    "    \n",
    "class ReteNeurale:\n",
    "    def __init__(self, n_input: int, n_output: int = 1, lr: float = 0.01):\n",
    "        # IMPORTANTE!!! L'output di un layer deve essere uguale all'input del successivo\n",
    "        self.n_output = n_output\n",
    "        self.layers = [Layer(n_input, 5), Layer(5, n_output, '')]\n",
    "        self.lr = lr\n",
    "    \n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        y = x \n",
    "        for layer in self.layers:\n",
    "            y = layer.predict(y)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, pred: float, true: float) -> float:\n",
    "        return (pred-true)**2\n",
    "    \n",
    "    def __grad(self, pred: float, true: float) -> float:\n",
    "        return 2*(pred-true)\n",
    "    \n",
    "    def backwards(self, pred: float, true: float):\n",
    "        grad = np.ones((1, self.n_output))*self.__grad(pred, true)\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.update(grad, self.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui bisogna fare particolare attenzione: il primo termine del gradiente è la derivata della loss function rispetto all'output. Questo termine viene calcolato dalla rete e utilizzato negli step successivi. Ogni layer poi aggiorna i propri parametri basandosi sul gradiente del layer precedente nell'ordine a ritroso. Nel nostro caso, il layer di output, contentente un neurone, prende il gradiente della loss function e lo moltiplica per la derivata dell'attivazione (1). Per aggiornare i suoi parametri moltiplica per l'input $a_i$ per calcolare gli $w_{oi}$ e per 1 per calcolare gli $b_{oi}$. Passa poi al layer precedente i gradienti rispetto a $a_i$.\n",
    "\n",
    "Questo procedimento che parte dalla fine e propaga le derivate verso i primi layer è chiamato *backpropagation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora proviamo ad allenare la nostra rete sui dati a disposizione. Prima però procediamo a *normalizzare* i nostri dati. Vediamo che significa. \n",
    "I nostri dati hanno delle distribuzioni che variano da caratteristica a caratteristica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "ax[0][0].hist(iris['sepal.length'])\n",
    "ax[0][0].set_title('Lunghezza del sepalo')\n",
    "ax[0][1].hist(iris['sepal.width'])\n",
    "ax[0][1].set_title('Larghezza del sepalo')\n",
    "ax[1][0].hist(iris['petal.length'])\n",
    "ax[1][0].set_title('Lunghezza del sepalo')\n",
    "ax[1][1].hist(iris['petal.width'])\n",
    "ax[1][1].set_title('Larghezza del sepalo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vedi, i dati non sono tutti nello stesso intervallo, e non hanno neanche la stessa media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(iris['sepal.length']), np.mean(iris['sepal.width']), np.mean(iris['petal.length']), np.mean(iris['petal.width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reti neurali imparano meglio se i dati sono tutti normalizzati in uno stesso intervallo, perché così nessuno caratteristica è favorita semplicemente per la scelta del valore. Inoltre, avendo a che fare con float, l'intervalli intorno all' 1 (o -1) hanno sempre una precisione migliore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La scelta della normalizzazione dipende un po' dal problema. Sono comuni la normalizzazione all'intervallo [0, 1], che si ottiene dalla relazione:  \n",
    "\n",
    "$\\frac{x - \\min}{\\max - \\min}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'alternativa, che useremo qui, è di sottrarre la media (che centra la distribuzione in 0) e dividere per la deviazione standard:  \n",
    "\n",
    " $\\sigma = \\sqrt(<x^2>-<x>^2)$\n",
    " \n",
    "In pratica l'espressione sotto radice è la differenza tra la media dei quadrati e il quadrato della media, e dà un'indicazione di quanto i dati siano lontani dalla media. Vediamo in pratica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dati = np.stack((iris['sepal.length'], iris['sepal.width'], iris['petal.length'], iris['petal.width']))\n",
    "dati.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(dati, axis=1, keepdims=True)\n",
    "std = np.std(dati, axis=1, keepdims=True)\n",
    "dati_norm = (dati - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "ax[0][0].hist(dati_norm[0])\n",
    "ax[0][0].set_title('Lunghezza del sepalo')\n",
    "ax[0][1].hist(dati_norm[1])\n",
    "ax[0][1].set_title('Larghezza del sepalo')\n",
    "ax[1][0].hist(dati_norm[2])\n",
    "ax[1][0].set_title('Lunghezza del sepalo')\n",
    "ax[1][1].hist(dati_norm[3])\n",
    "ax[1][1].set_title('Larghezza del sepalo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche i dati *target* (le classi) devono essere elaborati, soprattutto considerando che sono delle stringe. Useremo la mappatura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Setosa':0, 'Versicolor': 1, 'Virginica': 2}\n",
    "y = np.array([mapping[value] for value in iris['variety']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, non vogliamo che la nostra rete impari solo a classificare i dati che ha visto durante l'allenamento, ma che possa poi generalizzare a dati non visti. Dividiamo dunque il nostro dataset in dati per l'allenamento (training) e test (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([dati[:, :40], dati[:, 50:90], dati[:, 100:140]], axis=1)\n",
    "y_train = np.concatenate([y[:40], y[50:90], y[100:140]])\n",
    "x_test = np.concatenate([dati[:,40:50], dati[:,90:100], dati[:,140:150]], axis=1)\n",
    "y_test = np.concatenate([y[40:50], y[90:100], y[140:150]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora alleniamo la rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ReteNeurale(4)\n",
    "loss = {}\n",
    "accuracy = {'train':{}, 'test': {}}\n",
    "n_epochs = 1000\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    for indice in range(x_train.shape[1]):\n",
    "        y_pred = network.predict(x_train[:, indice])\n",
    "        loss.update({epoch+1: network.loss(y_pred, y_train[indice])})\n",
    "        network.backwards(y_pred, y_train[indice])\n",
    "        \n",
    "    accuracy_epoch_train = 0\n",
    "    for indice in range(x_train.shape[1]):\n",
    "        y_pred = network.predict(x_train[:, indice])\n",
    "        accuracy_epoch_train += 1 if np.abs(y_pred-y_train[indice])[0] < 0.5 else 0.0\n",
    "    accuracy_epoch_train /= x_train.shape[1]\n",
    "    accuracy['train'].update({epoch+1: accuracy_epoch_train})\n",
    "    \n",
    "    accuracy_epoch_test = 0\n",
    "    for indice in range(x_test.shape[1]):\n",
    "        y_pred = network.predict(x_test[:, indice])\n",
    "        accuracy_epoch_test += 1 if np.abs(y_pred-y_test[indice])[0] < 0.5 else 0.0\n",
    "    accuracy_epoch_test /= x_test.shape[1]\n",
    "    accuracy['test'].update({epoch+1: accuracy_epoch_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(10, 30))\n",
    "ax[0].scatter(loss.keys(), loss.values())\n",
    "ax[1].scatter(accuracy['train'].keys(), accuracy['train'].values())\n",
    "ax[2].scatter(accuracy['test'].keys(), accuracy['test'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
